# Server
PORT=3000
HOST=0.0.0.0

# Database
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/agentim

# Redis
REDIS_URL=redis://localhost:6379

# JWT (REQUIRED: generate with `openssl rand -base64 32`)
JWT_SECRET=change-me-to-a-random-secret
JWT_ACCESS_EXPIRY=15m
JWT_REFRESH_EXPIRY=7d

# Admin (seeded on startup, REQUIRED in Docker)
ADMIN_USERNAME=admin
ADMIN_PASSWORD=ChangeMe123!

# File Uploads
UPLOAD_DIR=./uploads
MAX_FILE_SIZE=10485760

# Storage Provider: 'local' (default) or 's3' (also works with R2, MinIO)
# STORAGE_PROVIDER=local
# S3_BUCKET=
# S3_REGION=auto
# S3_ENDPOINT=          # For R2/MinIO: https://xxx.r2.cloudflarestorage.com
# S3_ACCESS_KEY_ID=
# S3_SECRET_ACCESS_KEY=

# CORS
CORS_ORIGIN=http://localhost:5173

# Trust proxy headers (X-Forwarded-For, X-Real-IP) for client IP resolution.
#
# IMPORTANT: Set this to "true" when deploying behind ANY reverse proxy, including:
#   - Railway, Render, Northflank (all inject X-Forwarded-For automatically)
#   - Nginx, Caddy, Traefik, Cloudflare Tunnel, AWS ALB
#
# When set to "false" (default), ALL requests appear to come from the proxy's IP address.
# This causes rate limiting to be completely ineffective — every user shares the same
# rate limit bucket (the proxy IP), so a single user can exhaust the limit for everyone.
#
# When set to "true", the real client IP from X-Forwarded-For / X-Real-IP is used.
# Only enable this when you fully control and trust the upstream proxy.
# Enabling it without a proxy allows clients to spoof their IP and bypass rate limits.
#
# Recommendation: Railway / Northflank / Render one-click deploy templates should
# default this to "true" since they always sit behind a load balancer.
# Set to "false" only for bare-metal / direct-internet deployments.
TRUST_PROXY=false

# Sentry (optional - leave empty to disable)
SENTRY_DSN=

# REQUIRED in production, min 32 chars — generate with: openssl rand -base64 32
# Without this, Router API keys are stored in plaintext — must be exactly 32 bytes (base64-encoded)
ENCRYPTION_KEY=

# AI Router (optional — uses OpenAI-compatible API, works with Groq/OpenAI/etc.)
# Without this, broadcast rooms only route via @mentions
ROUTER_LLM_BASE_URL=
ROUTER_LLM_API_KEY=
ROUTER_LLM_MODEL=
# ROUTER_LLM_TIMEOUT_MS=15000   # LLM request timeout in ms (default 15000, min 1000)

# Routing protection
MAX_AGENT_CHAIN_DEPTH=5
AGENT_RATE_LIMIT_WINDOW=60
AGENT_RATE_LIMIT_MAX=20

# Cleanup & rate limiting (intervals in milliseconds, minimum 60000)
# ORPHAN_FILE_CHECK_INTERVAL=3600000
# TOKEN_CLEANUP_INTERVAL=3600000

# Client WebSocket rate limiting
# CLIENT_RATE_LIMIT_WINDOW=10    # Window in seconds (1-300)
# CLIENT_RATE_LIMIT_MAX=30       # Max messages per window (1-1000)

# WebSocket connection limits
# MAX_WS_CONNECTIONS_PER_USER=10
# MAX_TOTAL_WS_CONNECTIONS=5000
# MAX_GATEWAYS_PER_USER=20

# Log level: debug | info | warn | error | fatal
# Default: "debug" in development, "info" in production
# LOG_LEVEL=info
